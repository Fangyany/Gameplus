{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from GameFormer.predictor import GameFormer\n",
    "from torch.utils.data import DataLoader\n",
    "from GameFormer.train_utils import *\n",
    "\n",
    "num_neighbors = 20\n",
    "batch_size = 32\n",
    "# set up data loaders\n",
    "train_path = '/data/fyy/GameFormer-Planner/nuplan/processed_data/train'\n",
    "train_files = [f for d in os.listdir(train_path) for f in glob.glob(os.path.join(train_path, d, \"*.npz\"))]\n",
    "train_set = DrivingData(train_files, num_neighbors)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1690 [00:02<?, ?batch/s]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "with tqdm(train_loader, desc=\"Training\", unit=\"batch\") as data_epoch:\n",
    "    for batch in data_epoch:\n",
    "        count += 1\n",
    "        if count == 2:\n",
    "            break\n",
    "        # prepare data\n",
    "        inputs = {\n",
    "            'ego_agent_past': batch[0].to('cuda'),\n",
    "            'neighbor_agents_past': batch[1].to('cuda'),\n",
    "            'map_lanes': batch[2].to('cuda'),\n",
    "            'map_crosswalks': batch[3].to('cuda'),\n",
    "            'route_lanes': batch[4].to('cuda')\n",
    "        }\n",
    "\n",
    "        ego_future = batch[5].to('cuda')\n",
    "        neighbors_future = batch[6].to('cuda')\n",
    "        neighbors_future_valid = torch.ne(neighbors_future[..., :2], 0)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agent encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GameFormer.predictor_modules import *\n",
    "\n",
    "# shape = (32, T, 7) = (x, y, heading, vx, vy, ax, ay)\n",
    "ego = inputs['ego_agent_past']   \n",
    "# shape = (32, N, T, 11) = (x, y, heading, vx, vy, yaw, length, width, 1, 0, 0)   \n",
    "neighbors = inputs['neighbor_agents_past']    \n",
    "# shape = (32, 1+N, T, 5) = (x, y, heading, vx, vy)\n",
    "actors = torch.cat([ego[:, None, :, :5], neighbors[..., :5]], dim=1)\n",
    "\n",
    "ego_encoder = AgentEncoder(agent_dim=7).cuda()  # LSTM\n",
    "# shape = (32, 256)\n",
    "encoded_ego = ego_encoder(ego)\n",
    "\n",
    "agent_encoder = AgentEncoder(agent_dim=11).cuda()  # LSTM\n",
    "# shape = (N, 32, 256)\n",
    "encoded_neighbors = [agent_encoder(neighbors[:, i]) for i in range(neighbors.shape[1])]\n",
    "\n",
    "# shape = (32, N+1, 256)\n",
    "encoded_actors = torch.stack([encoded_ego] + encoded_neighbors, dim=1)  \n",
    "actors_mask = torch.eq(actors[:, :, -1].sum(-1), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lane_len = 50\n",
    "_lane_feature = 7\n",
    "_crosswalk_len = 30\n",
    "_crosswalk_feature = 3\n",
    "\n",
    "map_lanes = inputs['map_lanes']    # shape = (32, 40, 50, 7)\n",
    "map_crosswalks = inputs['map_crosswalks']   # shape = (32, 5, 30, 3)\n",
    "\n",
    "lane_encoder = VectorMapEncoder(_lane_feature, _lane_len).cuda()\n",
    "# shape = (32, 200, 256)\n",
    "encoded_map_lanes, lanes_mask = lane_encoder(map_lanes)\n",
    "\n",
    "crosswalk_encoder = VectorMapEncoder(_crosswalk_feature, _crosswalk_len).cuda()\n",
    "# shape = (32, 15, 256)\n",
    "encoded_map_crosswalks, crosswalks_mask = crosswalk_encoder(map_crosswalks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention fusion encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape = (32, 236, 256)\n",
    "input = torch.cat([encoded_actors, encoded_map_lanes, encoded_map_crosswalks], dim=1)\n",
    "mask = torch.cat([actors_mask, lanes_mask, crosswalks_mask], dim=1)\n",
    "\n",
    "dim, layers, heads, dropout = 256, 6, 8, 0.1\n",
    "attention_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=dim*4,\n",
    "                                                activation='gelu', dropout=dropout, batch_first=True)\n",
    "fusion_encoder = nn.TransformerEncoder(attention_layer, layers).cuda()\n",
    "encoding = fusion_encoder(input, src_key_padding_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = {\n",
    "    'actors': actors,\n",
    "    'encoding': encoding,\n",
    "    'mask': mask,\n",
    "    'route_lanes': inputs['route_lanes']\n",
    "}\n",
    "# torch.Size([32, 21, 21, 5]) torch.Size([32, 236, 256]) torch.Size([32, 236]) torch.Size([32, 10, 50, 3])\n",
    "# torch.Size([32, 21, 21, 5]) torch.Size([32, 236, 256]) torch.Size([32, 236]) torch.Size([32, 10, 50, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 21, 21, 5]) torch.Size([32, 236, 256]) torch.Size([32, 236]) torch.Size([32, 10, 50, 3])\n"
     ]
    }
   ],
   "source": [
    "print(actors.shape, encoding.shape, mask.shape, inputs['route_lanes'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = {}\n",
    "\n",
    "# shape = (32, 1+N, 5)\n",
    "current_states = encoder_outputs['actors'][:, :, -1]\n",
    "# shape = (32, 200+15+21, 256)\n",
    "encoding, mask = encoder_outputs['encoding'], encoder_outputs['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = 10\n",
    "modalities = 6\n",
    "levels = 3\n",
    "\n",
    "class GMMPredictor(nn.Module):\n",
    "    def __init__(self, modalities=6):\n",
    "        super(GMMPredictor, self).__init__()\n",
    "        self.modalities = modalities\n",
    "        self._future_len = 80\n",
    "        self.gaussian = nn.Sequential(nn.Linear(256, 512), nn.ELU(), nn.Dropout(0.1), nn.Linear(512, self._future_len*4))\n",
    "        self.score = nn.Sequential(nn.Linear(256, 64), nn.ELU(), nn.Linear(64, 1))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        B, N, M, _ = input.shape\n",
    "        traj = self.gaussian(input).view(B, N, M, self._future_len, 4) # mu_x, mu_y, log_sig_x, log_sig_y\n",
    "        score = self.score(input).squeeze(-1)\n",
    "\n",
    "        return traj, score\n",
    "\n",
    "\n",
    "class InitialPredictionDecoder(nn.Module):\n",
    "    def __init__(self, modalities, neighbors, dim=256):\n",
    "        super(InitialPredictionDecoder, self).__init__()\n",
    "        self._modalities = modalities\n",
    "        self._agents = neighbors + 1\n",
    "        self.multi_modal_query_embedding = nn.Embedding(modalities, dim)\n",
    "        self.agent_query_embedding = nn.Embedding(self._agents, dim)\n",
    "        self.query_encoder = CrossTransformer()\n",
    "        self.predictor = GMMPredictor()\n",
    "        self.register_buffer('modal', torch.arange(modalities).long())\n",
    "        self.register_buffer('agent', torch.arange(self._agents).long())\n",
    "\n",
    "    def forward(self, current_states, encoding, mask):\n",
    "        N = self._agents   # N = 1 + 10 = 11\n",
    "        multi_modal_query = self.multi_modal_query_embedding(self.modal)   # 可学习的嵌入向量\n",
    "        # self.modal.shape = (6)\n",
    "        # multi_modal_query.shape = (6, 256)\n",
    "        agent_query = self.agent_query_embedding(self.agent)\n",
    "        # self.agent.shape = (N)\n",
    "        # agent_query.shape = (N, 256)\n",
    "        query = encoding[:, :N, None] + multi_modal_query[None, :, :] + agent_query[:, None, :]\n",
    "        query_content = torch.stack([self.query_encoder(query[:, i], encoding, encoding, mask) for i in range(N)], dim=1)\n",
    "        # query.shape = (32, N, M, 256)\n",
    "        # query_content.shape = (32, N, M, 256)\n",
    "        predictions, scores = self.predictor(query_content)\n",
    "        # predictions.shape = (32, N, M, 4 * T, 4), mu_x, mu_y, log_sig_x, log_sig_y\n",
    "        # scores.shape = (32, N, M)\n",
    "        predictions[..., :2] += current_states[:, :N, None, None, :2]\n",
    "\n",
    "        return query_content, predictions, scores\n",
    "    \n",
    "initial_predictor = InitialPredictionDecoder(modalities, neighbors).cuda()\n",
    "last_content, last_level, last_score = initial_predictor(current_states, encoding, mask)\n",
    "content_list = [last_content]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 0 decode\n",
    "last_content, last_level, last_score = initial_predictor(current_states, encoding, mask)\n",
    "decoder_outputs['level_0_interactions'] = last_level\n",
    "decoder_outputs['level_0_scores'] = last_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6, 256])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_content[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_encoder = FutureEncoder().cuda()\n",
    "# 把这个网络复制2份\n",
    "interaction_stage = nn.ModuleList([InteractionDecoder(modalities, future_encoder).cuda() for _ in range(levels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# level k reasoning\n",
    "for k in range(1, 3):\n",
    "    interaction_decoder = interaction_stage[k-1]\n",
    "    last_content, last_level, last_score = interaction_decoder(current_states, last_level, last_score, last_content, encoding, mask)\n",
    "    decoder_outputs[f'level_{k}_interactions'] = last_level\n",
    "    decoder_outputs[f'level_{k}_scores'] = last_score\n",
    "    content_list.append(last_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 11, 6, 3, 256])\n",
      "torch.Size([32, 11, 6, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ContentFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim):\n",
    "        super(ContentFeatureExtractor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class FeatureFusionAttention(nn.Module):\n",
    "    def __init__(self, feature_size, num_features):\n",
    "        super(FeatureFusionAttention, self).__init__()\n",
    "        self.query = nn.Linear(feature_size, feature_size)\n",
    "        self.key = nn.Linear(feature_size, feature_size)\n",
    "        self.value = nn.Linear(feature_size, feature_size)\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 的形状是 [batch_size, num_tensors, seq_len, num_features, feature_size]\n",
    "        batch_size, num_tensors, seq_len, num_features, feature_size = x.shape\n",
    "        \n",
    "        # 将x变形为 [batch_size * seq_len * num_features, num_tensors, feature_size]\n",
    "        x_reshaped = x.view(-1, num_tensors, feature_size)\n",
    "        \n",
    "        # 计算查询（Q）、键（K）和值（V）\n",
    "        Q = self.query(x_reshaped)\n",
    "        K = self.key(x_reshaped)\n",
    "        V = self.value(x_reshaped)\n",
    "        # 计算注意力分数\n",
    "        attention_scores = Q @ K.transpose(-2, -1) / (feature_size ** 0.5)\n",
    "        \n",
    "        # 应用softmax函数获取注意力权重\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # 应用注意力权重到V（值）\n",
    "        attended = attention_weights @ V\n",
    "        # 将结果变形回 [batch_size, seq_len, num_features, num_tensors, feature_size]\n",
    "        attended = attended.view(batch_size, seq_len, num_features, num_tensors, feature_size)\n",
    "        print(attended.shape)\n",
    "        # 在num_tensors维度上求和或者平均\n",
    "        fused_features = attended.sum(dim=3)  # 使用 mean(dim=3) 来取平均\n",
    "        print(fused_features.shape)\n",
    "        return fused_features\n",
    "    \n",
    "content_feature_extractor = ContentFeatureExtractor(256, 256).cuda()\n",
    "feature_fusion_attention = FeatureFusionAttention(feature_size=256, num_features=6).cuda()\n",
    "\n",
    "content_dim = []\n",
    "for input in content_list:\n",
    "    batch_size, num_v, modalities, feature_dim = input.shape\n",
    "    out = content_feature_extractor(input.view(-1, feature_dim)).view(batch_size, num_v, modalities, -1)\n",
    "    content_dim.append(out)\n",
    "    \n",
    "combined_feature_tensors = torch.stack((content_dim[0], content_dim[1], content_dim[2]), dim=1) \n",
    "attention_output = feature_fusion_attention(combined_feature_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 11, 6, 256])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(content_dim[0] + content_dim[1] + content_dim[2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 11, 6, 256])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 11, 6, 256])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_feature_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttPredictionDecoder(nn.Module):\n",
    "    def __init__(self, neighbors):\n",
    "        super(AttPredictionDecoder, self).__init__()\n",
    "        self._agents = neighbors + 1\n",
    "        self.predictor = GMMPredictor()\n",
    "\n",
    "    def forward(self, current_states, att_content):\n",
    "        N = self._agents\n",
    "        predictions, scores = self.predictor(att_content)\n",
    "        predictions[..., :2] += current_states[:, :N, None, None, :2]\n",
    "        return att_content, predictions, scores\n",
    "\n",
    "att_predictor = AttPredictionDecoder(neighbors).cuda()\n",
    "att_content, predictions, scores = att_predictor(current_states, attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6, 256])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_content[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 11, 6, 80, 4])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuplan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
